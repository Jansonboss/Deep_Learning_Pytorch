{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "virgin-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-retrieval",
   "metadata": {},
   "source": [
    "In this notebook we will go over DL fundamental concept for pytorch\n",
    "- Understanding torch.nn.Linear\n",
    "- Understanding torch.nn.Embedding\n",
    "- Axis and dim for tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-parade",
   "metadata": {},
   "source": [
    "# torch.nn.Linear\n",
    "\n",
    "`nn.Linear(5, 3)` creates a linear transformation that map  $R^5$  → $R^3$ ⇒ which means that `nn.linear(5, 3)` will initialize a weight matrix with shape (3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-maker",
   "metadata": {},
   "source": [
    "Furthermore, `nn.Linear(5, 3)` creates a linear transformation A where transform the matrix X with $A\\cdot X+b$ . In other words, A will transform (N, 5) matrix into a (N, 3) matrix, where N can be anything (number of observations). Where 5 is input feature number and 3 is the output feature number\n",
    "\n",
    "![example](img/Linear_Transformation.png)\n",
    "\n",
    "Notice that in Linear algebra class, we treat  $R^5$ as column vector like above but in pytorch and tensorflow we treat them as row vector: `[x1, x2, x3, x4, x5]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daily-london",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "W = nn.Linear(5, 3) # w matrix shape(3, 5)\n",
    "\n",
    "W.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confirmed-wagon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42570674,  0.20171326, -0.19392422, -0.06542477, -0.05925357],\n",
       "       [ 0.02411893, -0.1895127 ,  0.22421086,  0.01469514, -0.06337062],\n",
       "       [-0.00068069, -0.17809209, -0.06550798,  0.43950218,  0.2640894 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert back to numpy, detached() if tensor requires grad\n",
    "W.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-purchase",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(100, 5) # R^5\n",
    "\n",
    "\n",
    "Y = nn.Linear(x)\n",
    "print(Y.weight.shape) # R^3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-grenada",
   "metadata": {},
   "source": [
    "![example](img/nnLinear_mult.png)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-period",
   "metadata": {},
   "source": [
    "# nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-structure",
   "metadata": {},
   "source": [
    "given a list of ids we can \"look up\" the embedding corresponing to each id\n",
    "can you see that some vectors are the same?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "brave-mileage",
   "metadata": {},
   "source": [
    "embed = nn.Embedding(10, 4, padding_idx=0)\n",
    "embed.weight\n",
    "\n",
    "\n",
    "Parameter containing:\n",
    "\n",
    "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
    "        [ 1.1654,  1.5980, -0.2816,  0.9185],\n",
    "        [ 0.5757, -2.0648, -0.8724,  1.5125],\n",
    "        [ 1.7381,  0.0021, -0.0120,  1.1288],\n",
    "        [-0.7991,  0.2798,  0.0891, -0.0050],\n",
    "        [ 0.8562, -0.0507, -0.7133, -0.8885],\n",
    "        [-0.8830,  0.4114, -0.2243, -0.7651],\n",
    "        [ 0.3272, -0.4005, -1.2199, -0.5495],\n",
    "        [ 0.0119, -0.5974,  0.8222,  0.3510],\n",
    "        [-0.5812,  0.5939, -2.0563,  0.0662]], requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "a = torch.LongTensor([[1,4,1,5,1,0]])\n",
    "embed(a)\n",
    "\n",
    "\n",
    "tensor([[[ 1.1654,  1.5980, -0.2816,  0.9185],\n",
    "         [-0.7991,  0.2798,  0.0891, -0.0050],\n",
    "         [ 1.1654,  1.5980, -0.2816,  0.9185],\n",
    "         [ 0.8562, -0.0507, -0.7133, -0.8885],\n",
    "         [ 1.1654,  1.5980, -0.2816,  0.9185],\n",
    "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>)e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-uncle",
   "metadata": {},
   "source": [
    "So from embed matrix, we pick row `1, 4, 1, 5, 1, and 0` and then stak them all together\n",
    "\n",
    "This trick is heavily used in NLP. for example:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "authorized-memorial",
   "metadata": {},
   "source": [
    "``test``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-belly",
   "metadata": {},
   "source": [
    "## Code snippet\n",
    "\n",
    "The layer in your code snippet does essentially this:\n",
    "\n",
    "- creates two lookup tables in `__init__`\n",
    "- the layer is called with input of shape `(batch_size, 2)`:\n",
    "    - first column contains indices of user embeddings\n",
    "    - second columns contains indices of movies embeddings\n",
    "- those embeddings are multiplied and summed returning `(batch_size,)` (so it's different from `nn.Linear` which would return `(batch_size, out_features)` and do dot product instead of element-wise multiplication followed by summation like here)\n",
    "\n",
    "This is probably used to train both representations (of users and movies) for some recommender-like system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-privilege",
   "metadata": {},
   "source": [
    "## The Difference between nn.Linear and nn.Embedding\n",
    "\n",
    "> - 前者主要做线性加权，后者是embedding层，支持按索引检索\n",
    "> - 其实根本区别在于输入，nn.Linear的输入为一个向量，输出也为一个向量，向量的各个维的元素的取值范围为连续的。而nn.Embedding的输入只能为离散值，只需要输入一个离散值也能获取结果，而这个离散值实际上相当于取one-hot之后的向量(look up table)。\n",
    "\n",
    "In nn.Embeddings back propagation wouldnt happen on the entire matrix. Back propagation will be done only on the rows of the embedding matrix whose indices are passed.\n",
    "\n",
    "So, less computation is required in this case since we only do paramter update on  embedding matrix whose indices are passed `(Look up table)`\n",
    "\n",
    "At theoretical level, the embedding layer is a linear layer, there is not any difference at all. However, in practice, if you are building a deep learning software, you have to make a difference among them. This is because it does not make sense to apply an embedding layer using traditional matrix multiplication, as the input matrix is very sparse. For this reason, it is faster to do a look-up, although in terms of theory it is equivalent to doing a matrix multiplication.\n",
    "\n",
    "Essentially everything. `[torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)` is a lookup table; essentially works the same as `torch.Tensor` but with a few twists (like possibility to use sparse embedding or default value at specified index)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-restoration",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-jurisdiction",
   "metadata": {},
   "source": [
    "# Dimension (axis = 0, 1, 2)\n",
    "\n",
    "https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-medium",
   "metadata": {},
   "source": [
    "The key to grasp how *dim* in PyTorch and *axis* in NumPy work was this paragraph from Aerin’s article:\n",
    "\n",
    "> The way to understand the “axis” of numpy sum is that it collapses the specified axis. So when it collapses the axis 0 (the row), it becomes just one row (it sums column-wise).\n",
    "\n",
    "Numpy and torcharray sum that Axis:\n",
    "\n",
    "`ndarrays` also have several features you’d expect from an n-dimensional array; each ndarray has n axes, indexed from 0, so that the first axis is 0, the second is 1, and so on. In particular, since we deal with 2D ndarrays often, we can think of axis = 0 as the rows and axis = 1 as the columns—see Figure 1-3.\n",
    "\n",
    "This is for 2D array:\n",
    "\n",
    "![example](img/2d_axis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-onion",
   "metadata": {},
   "source": [
    "This is for 3D array axis: `(0, 1, 2) => (batch_size, sentence_len, embedding_dim)`\n",
    "\n",
    "- 0: batch_size, 1: sentence_len, 2: embedding_dim\n",
    "- Each matrix is a numerical representation of sentence\n",
    "- If batch_size = 3, there're 3 sentences inside this batch\n",
    "\n",
    "![example](img/3d_axis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-olive",
   "metadata": {},
   "source": [
    "Here is the original Tensor with 3 dimensions shape = (3, 2, 3)\n",
    "\n",
    "\n",
    "\n",
    "# Sum Over Dim = 0\n",
    "\n",
    "since we sum over `dim = 0`, so `dim = 0 -> 3` will be summed and result will be `shape = (2, 3)`\n",
    "\n",
    "---\n",
    "```python\n",
    ">> torch.sum(y, dim=0)\n",
    "tensor([[ 3,  6,  9],\n",
    "        [12, 15, 18]])\n",
    "\n",
    "dimensionindex = (0, 1, 2)\n",
    "original shape = (3, 2, 3)\n",
    "\n",
    "new shape = (2, 3)\n",
    "# notice that the dimension index = 0 will be clipped so ouput dim = (2, 3)\n",
    "```\n",
    "\n",
    "\n",
    "![example](img/chrome-capture.gif)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-alberta",
   "metadata": {},
   "source": [
    "# Sum Over Dim = 1\n",
    "\n",
    "```python\n",
    ">> torch.sum(y, dim=1)\n",
    "tensor([[5, 7, 9],\n",
    "        [5, 7, 9],\n",
    "        [5, 7, 9]])\n",
    "```\n",
    "\n",
    "![example](img/dim1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-estonia",
   "metadata": {},
   "source": [
    "\n",
    "# Sum over Dim = 2\n",
    "\n",
    "![example](img/dim2.gif)\n",
    "\n",
    "```\n",
    ">> torch.sum(y, dim=2)tensor([[ 6, 15],\n",
    "        [ 6, 15],\n",
    "        [ 6, 15]])\n",
    "\n",
    "(3, 2, 3) -> (3 , 3) dim_index = 2(the last dimension index), which is 3 will be collasped\n",
    "\n",
    "so in the end shape = (3, 2) \n",
    "```\n",
    "\n",
    "Recall that we when we have a word embedding let's say `feature dimension = 4` like below:\n",
    "\n",
    "```python\n",
    "word_embedding = [\n",
    "\t\t\t\thello -0.121 -.431 .97712 .3343\n",
    "\t\t\t\tworld .....\n",
    "\t\t\t\tball  .12121 .23223 .2324 .5742\n",
    "\t\t\t\t\t\t\t\t\t] \n",
    "```\n",
    "\n",
    "and we use `np.mean(word_embedding, axis = 0)` which means we sum up across the row $\\sum_{i=1}^{n} x_{ij}$ (along the i axis → going down)and then take the average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-dealing",
   "metadata": {},
   "source": [
    "# Reference Link\n",
    "- https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
