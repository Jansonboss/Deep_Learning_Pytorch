{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be covering following topics:\n",
    "\n",
    "- Learning Rate Schedule\n",
    "- Dropout component\n",
    "- Normalization (BN, LN, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Learning Rate Schedule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple 2 layers NN\n",
    "model = nn.Sequential(\n",
    "\tnn.Linear(2, 5),\n",
    "\tnn.ReLU(),\n",
    "\tnn.Linear(5, 1)\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can group the parameters of our model into different groups (will be used for transfer learning later)\n",
    "\n",
    "- here we only have one group ( each group will be in one list)\n",
    "- note the learning rate below is 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[ 0.2197, -0.3686],\n",
       "           [-0.4251, -0.1346],\n",
       "           [-0.2628,  0.6199],\n",
       "           [-0.1249,  0.3726],\n",
       "           [ 0.0527,  0.5060]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.0067, -0.4347,  0.1457, -0.4703,  0.5562], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[-0.0317, -0.1654,  0.3326,  0.2197, -0.3974]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.3169], requires_grad=True)],\n",
       "  'lr': 0.0007810416889260659,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'weight_decay': 0,\n",
       "  'amsgrad': False,\n",
       "  'initial_lr': 0.001}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do you think we are getting an error here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.0009997532801828658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "# for every 100 iterations, we gonna change the learning rate\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 100)\n",
    "\n",
    "# why do you think we are getting an error here?\n",
    "print(optimizer.param_groups[0]['lr'])\n",
    "lr_scheduler.step()\n",
    "print(optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the optimizer.step() call is skipped and thus the learning rate scheduler is called before the first parameter update.\n",
    "We are working on a method to get the last status of the scaler. In the meantime you could try to check if the scale value was reduced and if so skip the learning rate scheduler step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "```raw\n",
    "model = [Parameter(torch.randn(2, 2, requires_grad=True))]\n",
    "optimizer = SGD(model, 0.1)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9) # optim.lr_scheduler.ExponentialLR()\n",
    "\n",
    "for epoch in range(20):\n",
    "    for input, target in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa150c62520>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_lists = []\n",
    "for i in range(500):\n",
    "\tlr_scheduler.step()\n",
    "\tlr = optimizer.param_groups[0][\"lr\"]\n",
    "\tlr_lists.append(lr)\n",
    "\n",
    "plt.plot(lr_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "During training, some number of layer outputs are randomly ignored or “dropped out.” This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different “view” of the configured layer.\n",
    "\n",
    "Dropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./img/dropout-capture.gif\" alt=\"drawing\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model with Dropout\n",
    "class TwoLayerNN_drop(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerNN_drop, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Add some dropout after first layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Why we need Dropout?\n",
    "\n",
    "- An unregularized network quickly overfits on the training dataset. Notice how the validation loss for without-dropout run diverges a lot after just a few epochs. This accounts for the higher generalization error. \n",
    "\n",
    "- Training with two dropout layers with a dropout probability of 25% prevents model from overfitting. However, this brings down the training accuracy, which means a regularized network has to be trained longer. \n",
    "\n",
    "- Dropout improves the model generalization. Even though the training accuracy is lower than the unregularized network, the overall validation accuracy has improved. This accounts for a lower generalization error. \n",
    "\n",
    "\n",
    "\n",
    "Some useful links:\n",
    "\n",
    "- https://zhang-yang.medium.com/scaling-in-neural-network-dropout-layers-with-pytorch-code-example-11436098d426\n",
    "\n",
    "- https://wandb.ai/authors/ayusht/reports/Dropout-in-PyTorch-An-Example--VmlldzoxNTgwOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How it looks like inside ?\n",
    "\n",
    "There're 2 steps inside the dropout\n",
    "\n",
    "Step1: The each element in the weight matrix follow Bernouli Distribution and have `p` probability to be zero-outed. And some elements in the weight matrix will be reduced to zero like below:\n",
    "\n",
    "<img src=\"./img/dropout_weights.png\" alt=\"drawing\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2: Scaled out other non-zero elements in the weight matrix proportionally with factor of `1 / (1  - p)`\n",
    "\n",
    "> During trraining, randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli Distribution. The elements to zeros are randomized on every forward call. And furthermore, the outputs are scaled by a factor of `1 / (1  - p)` during the training.\n",
    "\n",
    "\n",
    "This essentially is saying the layer `nn.Dropout(p)` will randomly zero out the elements on the input tensor with the probability `p`, the dropout rate. The result tensor will be scaled by `1 / (1 - p)` (divied by `(1 - p)`) during the training time\n",
    "\n",
    "Check it out: https://zhang-yang.medium.com/scaling-in-neural-network-dropout-layers-with-pytorch-code-example-11436098d426\n",
    "\n",
    "**and Let's see an example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]])\n",
      "tensor([[5., 0., 0.,  ..., 5., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 5., 0., 0.],\n",
      "        ...,\n",
      "        [5., 0., 0.,  ..., 0., 0., 5.],\n",
      "        [0., 0., 5.,  ..., 0., 0., 5.],\n",
      "        [5., 0., 5.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "drop = nn.Dropout(p = 0.8)\n",
    "x = torch.ones((1000, 1000))\n",
    "\n",
    "print(x)\n",
    "\n",
    "y = drop(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2002)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(y) / (1000*1000)\n",
    "# we can see roughly 20% of the element in tensor are non-zeros (80% are zero-outed)\n",
    "# and those 20% non-zero element would be scaled up from 1 to 5 <===> 1 / (1 - 0.8) = 5 (scaled it back )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why we need to scale the results during training?\n",
    "\n",
    "We can explain this by having an another seneraio in evalidation.\n",
    "\n",
    "> During the evaludation the modeule simple computes an identity functions. AKA during the evaluation time, the dropout layer will change nothing.\n",
    "\n",
    "Because `dropout` is active only during training time but not inference time, without the scaling, the expected output would be larger during inference time because the elements are no longer being randomly dropped (set to 0). But we want the expected output with and without going through the dropout layer to be the same. Therefore, during training, we compensate by making the output of the dropout layer larger by the scaling factor of `1/(1−p)`. A larger p means more aggressive dropout, which means the more compensation we need, i.e. the larger the scaling factor `1/(1−p)`.\n",
    "\n",
    "In other words, we we are rescaled the weights by adding extra noise without shifting the mean (mean = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(x.mean()) # during training before dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0010)\n"
     ]
    }
   ],
   "source": [
    "print(y.mean()) # during training after dropouted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that input x and output dropped y will be the roughly the same （这里都是training time)\n",
    "因为在inference的时候我们没有做zero out这个动作，会导致inference 的tensor数值要普遍大于 training的时候\n",
    "\n",
    "In Summary\n",
    "- `model.eval()` will keep the output at the same scale.\n",
    "- This basically says during evaluation/test/inference time, the dropout layer becomes an identity function and makes no change to its input.\n",
    "- Dropout component is really just adding noise with mean = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regularization (Weight Decay)**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blowing up weight decay so you can see it in action\n",
    "\n",
    "model = nn.Sequential(nn.Linear(2, 5),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(5, 1))\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, weight_decay = 1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5211,  0.4350],\n",
      "        [ 0.0138, -0.3197],\n",
      "        [-0.3029, -0.4422],\n",
      "        [ 0.6549, -0.0485],\n",
      "        [ 0.1896,  0.4058]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3095, -0.3481,  0.5380, -0.1477,  0.6348], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1884, -0.2489,  0.0130,  0.2549, -0.4154]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0845], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Before Weight Decay\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 3.2645e-04,  3.2642e-04],\n",
      "        [-9.3132e-10,  0.0000e+00],\n",
      "        [ 0.0000e+00,  2.9802e-08],\n",
      "        [ 4.4185e-04,  4.4187e-04],\n",
      "        [-7.1996e-04, -7.1999e-04]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 3.2645e-04,  2.9802e-08,  0.0000e+00,  4.4186e-04, -7.1996e-04],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1934e-03,  0.0000e+00, -9.3132e-10,  7.9492e-04,  2.1321e-03]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0017], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "y = model(torch.ones(10, 2))\n",
    "\n",
    "# train w.r.t a loss function that wants to maximize output\n",
    "(1/sum(y)).backward()\n",
    "optimizer.step()\n",
    "\n",
    "# weights have decreased\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Normalization**\n",
    "\n",
    "\n",
    "In a deep neural network, there is a phenomenon called internal **covariate shift**, which is a change in the input distribution to the network's layers due to the ever-changing network parameters during training.\n",
    "The input layer may have certain features which dominate the process, due to having high numerical values. This can create a bias in the network because only those features contribute to the outcome of the training. For example, imagine feature one having values between 1 and 5, and feature two having values between 100 and 10000. During training, due to the difference in scale of both features, feature two would dominate the network and only that feature would have a contribution to the outcome of the model.\n",
    "\n",
    "Normalization能够解决“Internal Covariate Shift”这种问题。简单理解就是随着层数的增加，中间层的输出会发生“漂移”。另外一种说法是：BN能够解决梯度弥散。通过将输出进行适当的缩放，可以缓解梯度消失的状况。\n",
    "\n",
    "\n",
    "\n",
    "Due to the reasons stated, a concept known as normalization was introduced to resolve these issues.\n",
    "\n",
    "\n",
    "- Reducing the internal covariate shift to improve training\n",
    "- Scaling each feature to a similar range to prevent or reduce bias in the network\n",
    "- Speeding up the optimization process by preventing weights from exploding all over the place and limiting them to a specific range\n",
    "- Reducing overfitting in the network by aiding in regularization (we must normalize before applying lasso, ridge and PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Normalization Methods**\n",
    "\n",
    "Ypu can check out this paper: https://arxiv.org/pdf/1803.08494.pdf\n",
    "\n",
    "\n",
    "There are several flavors of normalization developed over the years, which include:\n",
    "\n",
    "- Batch normalization\n",
    "- Layer normalization\n",
    "- Instance normalization\n",
    "- Group normalization\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./img/DL_Norm.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "Figure above. Normalization methods. Each subplot shows a feature map tensor, with `N` as the batch axis, `C` as the channel axis, and `(H, W)`\n",
    "as the spatial axes -> `height, and width` . The pixels in blue are normalized by the same `mean` and `variance`, computed by aggregating the values of these pixels.\n",
    "\n",
    "In NLP: `N` will be the batch axis, `C`, the Feature axis (word), and `(H, W)` will be the sentence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Batch normalization(BN)**\n",
    "\n",
    "<img src=\"./img/DL_BN.png\" alt=\"drawing\" width=\"700\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model with batch normalization\n",
    "class TwoLayerNN_BN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerNN_BN, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # we input the number of features to be normalizing across a batch\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        # add batch normalization before activation\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        # no batch norm for final output!\n",
    "        \n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Layer Normalization(LN)**\n",
    "\n",
    "<img src=\"./img/DL_LN.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/74516930\n",
    "\n",
    "Inspired by the results of Batch Normalization, Geoffrey Hinton et al. proposed Layer Normalization which normalizes the activations along the feature direction instead of mini-batch direction. This overcomes the cons of BN by removing the dependency on batches and makes it easier to apply for RNNs as well.\n",
    "\n",
    "In essence, Layer Normalization normalizes each feature of the activations to zero mean and unit variance. And since many each bacth of data will contain sentences with different lengths, Layer Normalizatiob can handle it more stably than batch normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "useful links:\n",
    "\n",
    "1. https://zhuanlan.zhihu.com/p/74516930\n",
    "2. https://arxiv.org/pdf/1803.08494.pdf\n",
    "3. https://ai-pool.com/a/s/normalization-in-deep-learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Early Stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
